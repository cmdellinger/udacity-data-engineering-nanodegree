{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# ETL pipline\n    \nIngests JSON files in 'input_data/log_data' and 'input_data/song_data' trees\ninto Spark DataFrames. The DataFrames are transformed into other DataFrames\nthat contain the columns corresponding to the schema in `README.md`. The\nprocessed data is then saved to parquet file trees in 'output_data/'"}, {"metadata": {}, "cell_type": "markdown", "source": "### Library Imports"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from datetime import datetime\nimport os\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf, col, monotonically_increasing_id\nfrom pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\nfrom pyspark.sql.functions import to_date, to_timestamp", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:red\">Required Information</span>"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# load AWS credentials into enviromental variables\nos.environ['AWS_ACCESS_KEY_ID']=\"AWS_ACCESS_KEY_ID\"\nos.environ['AWS_SECRET_ACCESS_KEY']=\"AWS_SECRET_ACCESS_KEY\"", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# input and output directories or S3 paths\n# note: don't use trailing '/' \ninput_data = \"s3a://sampleS3bucket/input_data\"\noutput_data = \"s3a:/sampleS3bucket/output_data\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Function Declarations"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def ts_msg(message):\n    \"\"\" prints string with a timestamp \"\"\"\n    timestamp = \"{} \".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n    print(timestamp + message)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def create_spark_session():\n    \"\"\" creates Spark session with AWS hadoop package \"\"\"\n    spark = SparkSession \\\n        .builder \\\n        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n        .getOrCreate()\n    return spark", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_song_data(spark, input_data, output_data):\n    \"\"\" Loads song data into songs and artists tables and saves them as parquet.\n        \n    The function ingests data into a Spark DataFrame from the song JSON files\n    located at 'input_data/song_data/'. The applicable data columns are parsed\n    and transformed based on the `songs` and `artists` table schema. The tables\n    are then save as parquet file/directory tree in the `output_data` directory.\n    \n    Args:\n        spark (SparkSession): SparkSession object of Spark cluster/instance\n        intput_data (str): root filepath containing `song_data` directory\n        output_data (str): root filepath of directory to save parquet file trees\n    Returns:\n        `None`: actions performed, but no return value\n    \"\"\"\n    \n    # get filepath to song data file\n    song_data = os.path.join(input_data, 'song_data/*/*/*/*.json')\n    \n    # read song data file\n    df = spark.read.json(song_data)\n    \n    # extract columns to create songs table\n    ts_msg(\"selecting songs_table\")\n    songs_table = df.select(\"song_id\", \\\n                            \"title\", \\\n                            \"artist_id\", \\\n                            \"year\", \\\n                            \"duration\") \\\n                    .dropDuplicates()\n            \n    # write songs table to parquet files partitioned by year and artist\n    ts_msg(\"writing songs_table\")\n    songs_table.write.parquet(os.path.join(output_data, \"songs_table\"), \\\n                              mode='overwrite', \\\n                              partitionBy=[\"year\",\"artist_id\"])\n\n    # extract columns to create artists table\n    ts_msg(\"selecting artists_table\")\n    artists_table = df.select('artist_id', \\\n                              col('artist_name').alias('name'), \\\n                              col('artist_location').alias('location'), \\\n                              col('artist_latitude').alias('latitude'), \\\n                              col('artist_longitude').alias('longitude')) \\\n                      .dropDuplicates()\n    \n    # write artists table to parquet files\n    ts_msg(\"writing artists_table\")\n    artists_table.write.parquet(os.path.join(output_data, \"artists_table\"), \\\n                                mode='overwrite')", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def process_log_data(spark, input_data, output_data):\n    \"\"\" Loads log data to users, time, & songplays tables and saves as parquet.\n        \n    The function ingests data into a Spark DataFrame from the log JSON files\n    located at 'input_data/log_data/'. The applicable data columns are parsed\n    and transformed based on the `users`, `time`, and `songplays` table schema.\n    The tables are then save as parquet file/directory tree in the `output_data`\n    directory.\n    \n    Args:\n        spark (SparkSession): SparkSession object of Spark cluster/instance\n        intput_data (str): root filepath containing `song_data` directory\n        output_data (str): root filepath of directory to save parquet file trees\n    Returns:\n        `None`: actions performed, but no return value\n    \"\"\"\n    # get filepath to log data file\n    log_data = os.path.join(input_data, 'log_data/*.json')\n    \n    # read log data file\n    df = spark.read.json(log_data)\n    \n    # filter by actions for song plays\n    df = df.filter(df.page == 'NextSong')\n    \n    # extract columns for users table\n    ts_msg(\"selecting users_table\")\n    users_table = df.select(col('userId').alias('user_id'), \\\n                            col('firstName').alias('first_name'), \\\n                            col('lastName').alias('last_name'), \\\n                            'gender', \\\n                            'level') \\\n                    .dropDuplicates()\n            \n    # write users table to parquet files\n    ts_msg(\"writing users_table\")\n    users_table.write.parquet(os.path.join(output_data, \"users_table\"), \\\n                               mode='overwrite')\n\n    # create timestamp column from original timestamp column\n    df = df.withColumn('timestamp', to_timestamp(df.ts/1000))\n\n    # create datetime column from original timestamp column\n    df = df.withColumn('datetime', to_date(df.timestamp))\n    \n    # extract columns to create time table\n    # start_time, hour, day, week, month, year, weekday\n    ts_msg(\"selecting time_table\")\n    time_table = df.select(col('timestamp').alias('start_time'), \\\n                           hour(col('datetime')).alias('hour'), \\\n                           dayofmonth(col('datetime')).alias('day'), \\\n                           weekofyear(col('datetime')).alias('week'), \\\n                           month(col('datetime')).alias('month'), \\\n                           year(col('datetime')).alias('year'), \\\n                           date_format(col('datetime'), 'E').alias('weekday')) \\\n                    .dropDuplicates()\n            \n    # write time table to parquet files partitioned by year and month\n    ts_msg(\"writing time_table\")\n    time_table.write.parquet(os.path.join(output_data, \"time_table\"), \\\n                             mode='overwrite', \\\n                             partitionBy=[\"year\",\"month\"])\n\n    # read in song data to use for songplays table\n    ts_msg(\"loading songs_table\")\n    song_df = spark.read.parquet(os.path.join(output_data, 'songs_table'))\n\n    # extract columns from joined song and log datasets to create songplays table\n    ts_msg(\"selecting songplays_table\")\n    songplays_table = df.join(song_df, df.song == song_df.title) \\\n                        .select(col('timestamp').alias('start_time'), \\\n                                col('userId').alias('user_id'), \\\n                                'level', \\\n                                'song_id', \\\n                                'artist_id', \\\n                                col('sessionId').alias('session_id'), \\\n                                'location', \\\n                                col('userAgent').alias('user_agent'), \\\n                                year('timestamp').alias('year'), \\\n                                month('timestamp').alias('month')) \\\n                        .dropDuplicates() \\\n                        .withColumn('songplay_id', monotonically_increasing_id())\n\n    # write songplays table to parquet files partitioned by year and month\n    ts_msg(\"writing songplays_table\")\n    songplays_table.write.parquet(os.path.join(output_data, \"songplays_table\"), \\\n                                  mode='overwrite', \\\n                                  partitionBy=[\"year\",\"month\"])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### ETL Execution"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "spark = create_spark_session()", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(\"===== processing song_data =====\")\nprocess_song_data(spark, input_data, output_data)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(\"===== processing log_data =====\")\nprocess_log_data(spark, input_data, output_data)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}