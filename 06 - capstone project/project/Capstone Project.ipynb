{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Summary of EPA Data Sets\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "<font color='red'>Note: To run this notebook an AWS key pair is required with S3 and Redshift read/write access as well as an existing Redshift cluster. The notebook pulls the information from the file `dwh.cfg`. The files in S3 will also need to be deleted after completion. <BR>**NO QUOTES in `dwh.cfg`**</font>\n",
    "\n",
    "#### Project Summary\n",
    "The project will gather several data sets from the EPA website and aggregate their entries for time series analysis, The ETL for this project will involve downloading, processing with Pandas, uploading to AWS S3, and loading and joining with AWS Redshift.\n",
    "\n",
    "<img src=\"images/pipeline.png\" alt=\"data pipeline\" style=\"width:80%\">\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "\n",
    "import requests\n",
    "import psycopg2\n",
    "import boto3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope \n",
    "The project will transform several hourly data sets from the EPA website into daily aggregated data suitable for time series analysis. The hourly data will be aggregated by state and day with summary statistics using Pandas and exported to CSV. The CSV files will be uploaded to an S3 bucket and `COPY`-ed into a Redshift database. The tables will then be `JOIN`-ed into tables containing the maximums and averages for each state and day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe and Gather Data \n",
    "The EPA website has daily, hourly, and 8-hour data for pollutants and meteorologic data aggregated by year. The data is contained in zipped CSV files with similar download URLs, varying by the EPA code and year, making it easy to programmatically download the data files. The ETL workflow will work for any of the hourly files over the course of years from 1980 to present:\n",
    "\n",
    "<img src=\"images/epa_page-criteria gases.png\" alt=\"epa_page-criteria gases\" style=\"width:50%\">\n",
    "<img src=\"images/epa_page-meteorological.png\" alt=\"epa_page-meteorological\" style=\"width:50%\">\n",
    "\n",
    "The default setup for this notebook will be download Ozone, SO2, CO, NO2, and Temperature for 2018 (the latest full year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dict = {'Ozone': 44201,\n",
    "             'SO2': 42401,\n",
    "             'CO': 42101,\n",
    "             'NO2': 42602,\n",
    "             'Temperature': 'TEMP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2018\n",
    "end = 2018\n",
    "years = range(start, end+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the files and the data in the CSV tables can be found at EPA website:\n",
    "    https://aqs.epa.gov/aqsweb/airdata/FileFormats.html\n",
    "An excerpt of that webpage specific to the hourly data files can be found in `AirData Download Files Documentation.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify Directory for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_name = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(folder_name):\n",
    "    # check that save directory exists, otherwise make directory\n",
    "    print(f\"checking for directory {folder_name}\")\n",
    "    if not os.path.exists(folder_name):\n",
    "        print(\"directory for data doesn't exist\")\n",
    "        print(f\"creating directory at {folder_name}\\n\")\n",
    "        os.makedirs(folder_name)\n",
    "    else:\n",
    "        print(\"data directory exists\\n\")\n",
    "\n",
    "def get_file_from_url(url, save_location=\"\"):\n",
    "    # check that save directory exists, otherwise make directory\n",
    "    create_directory(save_location)\n",
    "    # check for file name from url, otherwise download programatically\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(save_location, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"file {file_name} doesn't exist at {save_location}\")\n",
    "        print(f\"downloading {file_name} to {file_path}\\n\")\n",
    "        with open(file_path, mode = 'wb') as file:\n",
    "            file.write(requests.get(url).content)\n",
    "    else:\n",
    "        print(f'file {file_name} already exists at {file_path}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data directory\n",
    "create_directory(data_folder_name) \n",
    "\n",
    "# programatically download data files\n",
    "#\n",
    "# urls for hourly downloads are in the form:\n",
    "#    https://aqs.epa.gov/aqsweb/airdata/hourly_{CODE}_{YEAR}.zip\n",
    "# where code corresponds to a number or abbreviation.\n",
    "#\n",
    "# years imported above\n",
    "# years is a `range` of years\n",
    "# years = range(start, end+1)\n",
    "#\n",
    "# code_dict imported above\n",
    "# code_dict is a dictionary corresponding to names and EPA codes\n",
    "# example:\n",
    "#   code_dict = {'Temperature (62101)': 'TEMP',\n",
    "#                 'Ozone': 44201,\n",
    "#                 'SO2': 42401,\n",
    "#                 'CO': 42101,\n",
    "#                 'NO2': 42602}\n",
    "\n",
    "for title, code in code_dict.items():\n",
    "    for year in years:\n",
    "        working_dir = os.path.join(data_folder_name, title)\n",
    "        file_name = f'hourly_{code}_{year}.zip'\n",
    "        get_file_from_url(f'https://aqs.epa.gov/aqsweb/airdata/hourly_{code}_{year}.zip',\n",
    "                          working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data & Cleaning Steps\n",
    "Example data is explored in `data_exploration.ipynb`. The files examined are `hourly_TEMP_2016.zip` and `hourly_42101_2016.zip`, which are temperature and carbon monoxide data from 2016. The takeaways from examining both files are that sites record the exact longitude and latitude, which are diferent for every reading. There are different sites or multiple readings for each kind of data set, so the bucket size needs to be bigger to compare time series data. The bucket sizes chosen were grouping by state and using daily statistics. The data sets look like they're pretty clean in its current state, but grouping by state and aggragating to generate summary data will get rid of any missing or null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "A Redshift database was chosen for easy access to perform time series analysis. The final table schema will have the aggregated daily data (containing state, day (YYYY-MM-DD), count, min, max, mean, std) and a `JOIN`-ed table containing the daily maximums and minimums by state and day. \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "+ The data will be transformed locally using Pandas and saved to new CSV files.\n",
    "    + loading the data from zip files ended up being the slowest part because the zip files are extremely compressed and end up 2-3Gb.\n",
    "+ The new CSV files will be uploaded to S3 for faster loading into Redshift.\n",
    "    + saving to a new file and uploading the files and using `COPY` turned out to be faster than bulk `INSERT`.\n",
    "+ New mean and maximum tables will be created by joining all means and maximums from the daily data tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the Data to Daily Aggregations by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data in CSV files to aggregate values:\n",
    "#   state, day (YYYY-MM-DD), count, min, max, mean, std\n",
    "# then save to a new CSV file and upload to S3\n",
    "#\n",
    "# years imported above\n",
    "# years is a `range` of years\n",
    "# years = range(start, end+1)\n",
    "#\n",
    "# code_dict imported above\n",
    "# code_dict is a dictionary corresponding to names and EPA codes\n",
    "# example:\n",
    "#   code_dict = {'Temperature (62101)': 'TEMP',\n",
    "#                 'Ozone': 44201,\n",
    "#                 'SO2': 42401,\n",
    "#                 'CO': 42101,\n",
    "#                 'NO2': 42602}\n",
    "\n",
    "# get AWS credentials from config\n",
    "config = configparser.ConfigParser(allow_no_value=True)\n",
    "config.read('dwh.cfg')\n",
    "# create s3 session using AWS credentials\n",
    "session = boto3.Session(aws_access_key_id = config.get('AWS','KEY'),\n",
    "                    aws_secret_access_key= config.get('AWS','SECRET'))\n",
    "s3 = session.resource('s3')\n",
    "# get s3 bucket name to save files\n",
    "s3_bucket_name = config.get('S3','bucket_name')\n",
    "\n",
    "# get data file names and iterate through them\n",
    "for title, code in code_dict.items():\n",
    "    working_dir = os.path.join(data_folder_name, title)\n",
    "    files = map(lambda file_name: os.path.join(working_dir, file_name),\n",
    "                os.listdir(working_dir))\n",
    "    for file in files:\n",
    "        print(f'loading {file}')\n",
    "        df = pd.read_csv(file, dtype={'Qualifier': 'object',\n",
    "                                      'Date Local': 'object',\n",
    "                                      'Time Local': 'object',\n",
    "                                      'Date GMT': 'object',\n",
    "                                      'Time GMT': 'object'})\n",
    "        print(f'transforming {file}')\n",
    "        # drop all columns but 'State Name', 'Date Local', 'Sample Measurement'\n",
    "        df = df[['State Name', 'Date Local', 'Sample Measurement']]\n",
    "        # rename columns to one word for easier reference\n",
    "        df.rename(columns={\"State Name\": \"state\",\n",
    "                           \"Date Local\": \"date\",\n",
    "                           \"Sample Measurement\": title},\n",
    "                  inplace=True)\n",
    "        # group by and aggregate\n",
    "        # note: the aggregate function `agg` ended up being much faster than the `.describe()` method.\n",
    "        df = df.groupby(['state', 'date'], as_index=False)[title] \\\n",
    "               .agg([pd.Series.count, np.min, np.max, np.mean, np.std]) \\\n",
    "               .reset_index() \\\n",
    "               .rename(columns={\"amin\": \"min\",\n",
    "                                \"amax\": \"max\"}) \\\n",
    "               .astype({\"count\": \"int64\"})\n",
    "        # save transformed data and upload to S3\n",
    "        year = file.split('.')[-2][-4:]\n",
    "        transformed_file = f'daily_{title}_{year}.csv'\n",
    "        save_path = os.path.join(working_dir, transformed_file)\n",
    "        print(f'saving transformed data to {save_path}')\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f'uploading {save_path} to S3')\n",
    "        s3.Bucket(s3_bucket_name).upload_file(save_path, transformed_file)\n",
    "        # clean up local data files\n",
    "        print(f'deleting {file}')\n",
    "        os.remove(file)\n",
    "        print(f'deleting {save_path}')\n",
    "        os.remove(save_path)\n",
    "        print('')\n",
    "    \n",
    "# delete data folder   \n",
    "os.system(f'rm -r {data_folder_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser(allow_no_value=True)\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Tables (If They Exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_tables = list(map(lambda x: x.lower(), code_dict.keys())) \\\n",
    "            + ['measure_mean', 'measure_max']\n",
    "for table in drop_tables:\n",
    "    print(f'DROP TABLE IF EXISTS {table}')\n",
    "    cur.execute(f'DROP TABLE IF EXISTS {table}')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epa_codes = list(map(lambda x: x.lower(), code_dict.keys()))\n",
    "for table in epa_codes:\n",
    "    create_table_query = \\\n",
    "        f\"\"\"CREATE TABLE IF NOT EXISTS {table}\n",
    "            (state TEXT,\n",
    "             date DATE,\n",
    "             count INT,\n",
    "             min DECIMAL,\n",
    "             max DECIMAL,\n",
    "             mean DECIMAL,\n",
    "             std DECIMAL,\n",
    "             PRIMARY KEY (state, date)\n",
    "            );\n",
    "        \"\"\"\n",
    "    print(create_table_query)\n",
    "    cur.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "# build and commit queries for mean and max tables\n",
    "create_pivot_tables = ['measure_mean', 'measure_max']\n",
    "for table in create_pivot_tables:\n",
    "    # build query\n",
    "    create_pivot_table_query = \\\n",
    "        f\"\"\"CREATE TABLE IF NOT EXISTS {table}\n",
    "            (state TEXT, date DATE, \"\"\" \\\n",
    "            + \"\".join(map(lambda x: x + \" DECIMAL, \", epa_codes)) \\\n",
    "            + \"PRIMARY KEY (state, date));\"\n",
    "    # create table from query\n",
    "    print(create_pivot_table_query)\n",
    "    cur.execute(create_pivot_table_query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = config.get('AWS','KEY')\n",
    "secret_key = config.get('AWS','SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load daily tables\n",
    "for table in code_dict.keys():\n",
    "    daily_table_copy = (f\"\"\"\n",
    "        COPY {table.lower()}\n",
    "        FROM 's3://{s3_bucket_name}/daily_{table}'\n",
    "        access_key_id '{access_key}'\n",
    "        secret_access_key '{secret_key}'\n",
    "        FORMAT AS CSV\n",
    "        IGNOREHEADER 1;\n",
    "        \"\"\")\n",
    "    print(daily_table_copy)\n",
    "    cur.execute(daily_table_copy)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JOIN-ed tables\n",
    "tables = list(map(lambda x: x.lower(), code_dict.keys()))\n",
    "for agg_table in ['measure_mean', 'measure_max']:\n",
    "    query = f'INSERT INTO {agg_table} (state, \"date\", ' + \", \".join(tables) + \") \" + \\\n",
    "            f\"SELECT {tables[0]}.state, {tables[0]}.date, \" + \\\n",
    "            \", \".join(map(lambda x: x + \".{} as {}\".format(agg_table.split('_')[-1], x), tables)) + \\\n",
    "            f\" FROM {tables[0]} \" + \\\n",
    "            \" \".join(f\"JOIN {table} on ({table}.state = {tables[0]}.state and {table}.date = {tables[0]}.date)\" for table in tables[1:]) \n",
    "    print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser(allow_no_value=True)\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = sqlio.read_sql_query(\"\"\"SELECT * FROM PG_TABLE_DEF\"\"\", conn)\n",
    "schema = schema[schema.schemaname == 'public'][['tablename', 'column', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .head() to dipslay all of the table schema\n",
    "schema.groupby('tablename', as_index=False).head(max(2 + len(code_dict.keys()), 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check that Tables Loaded Into Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlio.read_sql_query(\"\"\"SELECT \"table\", tbl_rows FROM SVV_TABLE_INFO\"\"\", conn).astype({\"tbl_rows\": \"int64\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "The data will ultimately consist of tables of summary statistics by day and state. The summary statistics will be used to create tables of daily maximums and averages for all the tables. The final schema will depend on the what is indicated in `code_dict`, but a generic table schema can be seen below with the column types.\n",
    "\n",
    "<img src=\"images/schema.png\" alt=\"schema\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "Some of the rational of tools has been discussed throughout, but will be reviewed here. The end-product of a Redshift database was chosen due to scalability of size and access as well as ease of management as an AWS cloud product. Due to the data set chunk size of the zipped CSVs, the data set files can easily be unzipped and transformed using Pandas. Uploading the transformed data into S3 to use Redshift's `COPY` ability was chosen because it was ultimately faster than bulk `INSERT`-ing directly into the Redshift database using Pandas `to_sql` functionality or using the psycopg2 library directly. COPY is likely faster due to the parallelization and upload of transformed data is very fast due to the small file size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the notebook is flexible based on the `code_dict`, to download files for different gases/particulates and years. The notebook scans the directories, dynamically generates SQL code based on the `code_dict` and `years`, and Redshift uses partial, file name matching, so the amount of data can vary but the end result will be to have a daily summary table for each thing selected in `code_dict` and summary tables with daily averages and maximums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Addressing Pipeline Concerns\n",
    "If incremental updates need to be added (like daily updates), then there are several options:\n",
    " + [Air Quality System (AQS) API](https://aqs.epa.gov/aqsweb/documents/data_api.html): API access to data that will allow retreival of newest data.\n",
    " + [AirNow API](https://docs.airnowapi.org): Another provider that supplies real-time air quality data.\n",
    " + Use an Apache Airflow pipeline or a `cron` job with a Python script to redownload the zip file daily and only UPSERT the newest data.\n",
    "\n",
    "If the datasets required are increased 100x, then the pipeline needs to be changed. The current VM has no problem loading one dataset at a time for transformation, but directly loading the zip files into Redshift using `COPY` may be an option depending on the zip compression (only certain ones are supported). Alternatively, the AQS or AirNow API could also be used to obtain the data.\n",
    "\n",
    "If the dataset needs to be accessed by 100+ people, Redshift would have no problem scaling. It would require more money in user access, but all the heavy lifting will be done by AWS requiring only to grant user permissions with additional IAM roles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
